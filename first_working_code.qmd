---
title: "Spam Classification Ashley"
format: html
editor: visual
---

## Libraries

```{r, message = FALSE}
library(shiny)
library(bslib)
library(here)
library(dplyr)
library(plotly)
library(bayesrules)
library(tidyverse)
library(e1071)
library(janitor)
library(tm)
library(wordcloud)
library(tidytext)
```

## Data

```{r}
data <- read.table(file= here("~/R/DS400/finalproj/SMSSpamCollection.txt"), sep="\t", quote="", comment.char="")
```

## Cleaning

```{r}
head(data)
```

```{r}
colnames(data)
```

```{r}
colnames(data) <- c("type", "message")
colnames(data)
```

```{r}
head(data)
```

## Exploration

```{r}
ggplot(data = data, aes(x = type, fill = type)) + 
  geom_bar()
```

```{r}
count_exclamations <- sum(grepl("!", data$message))
count_exclamations
```

```{r}
data$has_exclamation <- ifelse(grepl("!", data$message), "yes", "no")
```

```{r}
ggplot(data, aes(x = type, fill = has_exclamation)) + 
  geom_bar()
```

Ham Word Cloud

```{r, warning=FALSE}
# Subset the data for 'ham' messages
ham_messages <- subset(data, type == "ham")$message

# Create a text corpus for 'ham' messages
ham_corpus <- Corpus(VectorSource(ham_messages))

# Clean the text: remove punctuation, stopwords, and convert to lowercase
ham_corpus <- tm_map(ham_corpus, content_transformer(tolower))
ham_corpus <- tm_map(ham_corpus, removePunctuation)
ham_corpus <- tm_map(ham_corpus, removeWords, stopwords("english"))
ham_corpus <- tm_map(ham_corpus, stripWhitespace)

# Create word cloud for 'ham' messages
wordcloud(ham_corpus, max.words = 120, random.order = FALSE, colors = brewer.pal(8, "Dark2"), main = "Ham Messages")
```

Spam Word Cloud

```{r, warning=FALSE}
# Subset the data for 'spam' messages
spam_messages <- subset(data, type == "spam")$message

# Create a text corpus for 'spam' messages
spam_corpus <- Corpus(VectorSource(spam_messages))

# Clean the text: remove punctuation, stopwords, and convert to lowercase
spam_corpus <- tm_map(spam_corpus, content_transformer(tolower))
spam_corpus <- tm_map(spam_corpus, removePunctuation)
spam_corpus <- tm_map(spam_corpus, removeWords, stopwords("english"))
spam_corpus <- tm_map(spam_corpus, stripWhitespace)

# Create word cloud for 'spam' messages with standard Reds palette
wordcloud(spam_corpus, max.words = 100, random.order = FALSE, colors = brewer.pal(8, "Reds"), main = "Spam Messages")
```

```{r}
# Add a new column 'word_count' to 'data' with the word count of each message
data$word_count <- sapply(strsplit(data$message, "\\s+"), length)
```

```{r}
ggplot(data, aes(x = type, y = word_count, color = has_exclamation)) + 
  geom_point()
```

It can be seen in this graph that spam messages are always less than 50 words in this data set. Also, a larger portion of spam has an exclamation point while the ham messages rarely do.

```{r}
# Count the number of spam messages with word_count less than 50
spam_count_greater_than_50 <- sum(data$type == "spam" & data$word_count > 50)
spam_count_greater_than_50
```

```{r}
# Next, run positive vs negative sentiment analysis
# Done by Lydia 
```

```{r, warning=FALSE}
# Load necessary libraries
library(tm)

# Subset the data for 'spam' messages
spam_messages <- subset(data, type == "spam")$message

# Create a text corpus for 'spam' messages
spam_corpus <- Corpus(VectorSource(spam_messages))

# Clean the text: remove punctuation, stopwords, and convert to lowercase
spam_corpus <- tm_map(spam_corpus, content_transformer(tolower))
spam_corpus <- tm_map(spam_corpus, removePunctuation)
spam_corpus <- tm_map(spam_corpus, removeWords, stopwords("english"))
spam_corpus <- tm_map(spam_corpus, stripWhitespace)

# Create a term-document matrix
tdm <- TermDocumentMatrix(spam_corpus)

# Convert term-document matrix to a matrix and get word frequencies
tdm_matrix <- as.matrix(tdm)
word_freqs <- rowSums(tdm_matrix)

# Sort word frequencies in decreasing order and get the top 10 words
top_10_words <- sort(word_freqs, decreasing = TRUE)[1:10]

# Convert to a data frame for readability
top_10_words_df <- data.frame(word = names(top_10_words), frequency = top_10_words, row.names = NULL)

# Display the list of top 10 words
top_10_words_df

```

```{r}
top_10 <- as.list(top_10_words_df$word)
top_10
```

```{r}
# Assuming 'top_10' is a list of the top 10 words as created previously
top_10_words <- unlist(top_10)  # Convert list to vector for pattern matching

# Create a new column 'contains_top_10' in 'data'
data$contains_top_10 <- sapply(data$message, function(msg) {
  # Check if any of the top 10 words are in the message
  if (any(sapply(top_10_words, function(word) grepl(word, msg, ignore.case = TRUE)))) {
    "yes"
  } else {
    "no"
  }
})

# Display the first few rows of data to verify
head(data)
```

## Bayes

```{r}
# Fake news example from class 
# naive_model_hints <- naiveBayes(type ~ title_words + negative + title_has_excl, data = fake_news)

# our_article <- data.frame(title_words = 15, negative = 6, title_has_excl = "FALSE")

# predict(naive_model_hints, newdata = our_article, type = "raw")

# fake_news <- fake_news %>% 
  #mutate(predicted_article = predict(naive_model_hints, newdata = .))

# fake_news %>% 
  #tabyl(type, predicted_article) %>% 
  #adorn_percentages("row") %>% 
  #adorn_pct_formatting(digits = 2) %>%
  #adorn_ns

# naive_model_hints
```

```{r}
spam_data <- 
  read.table(file = here("~/R/DS400/finalproj/SMSSpamCollection.txt"), sep="\t", quote="", comment.char="") %>%
  mutate(id = row_number()) %>% 
  select(id, everything())

colnames(spam_data)[c(2,3)] <- c("type", "message")
```

```{r}
#adding sentiment analysis by message

sentiment_by_message <- spam_data %>%
  unnest_tokens(word, message) %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(id, sentiment)

sentiment_by_message <- sentiment_by_message %>%
  spread(key = sentiment, value = n, fill = 0) %>%
  mutate(sentiment_score = positive - negative) %>%
  select(id, sentiment_score)

sentiment_by_message <- sentiment_by_message %>%
  mutate(sentiment = case_when(
    sentiment_score > 0 ~ "positive",
    sentiment_score < 0 ~ "negative",
    TRUE ~ "neutral"))
```

```{r}
spam_data <- spam_data %>% 
  left_join(sentiment_by_message, by = "id")
```

```{r}
spam_data
```

```{r}
spam_data <- spam_data %>% select(-sentiment_score)
```

```{r}
spam_data
```

```{r}
data <- cbind(data, spam_data)
```

```{r}
data
```

```{r}
# Drop columns 7 and 8 from 'data' using base R
data <- data[, -c(7, 8)]
```

```{r}
data
```

```{r}
data <- data %>%
  select(id, everything())
```

```{r}
data
```

Bayes

```{r}
naive_model_hints <- naiveBayes(type ~ has_exclamation + word_count + contains_top_10 + sentiment, data = data)

our_message <- data.frame(has_exclamation = 'yes', word_count = 15, contains_top_10 = 'no', sentiment = 'negative')

predict(naive_model_hints, newdata = our_message, type = "raw")

data <- data %>% 
mutate(predicted_type = predict(naive_model_hints, newdata = .))

data %>% 
  tabyl(type, predicted_type) %>% 
  adorn_percentages("row") %>% 
  adorn_pct_formatting(digits = 2) %>%
  adorn_ns

naive_model_hints
```

## Setting up the format of the app

```{r}
ui <- fluidPage(
  titlePanel("Spam Classification Model"),
  
  # Sidebar layout
  sidebarLayout(
    sidebarPanel(
      # allow for text input
      textInput("text", "Enter SMS text:"),
      
      # add an action button
      actionButton("classify", "Classify")
    ),
    
    # the main section of the app
    mainPanel(
      # output
      textOutput("prediction")
    )
  )
)
```

## creating the engine for the server

```{r}
server <- function(input, output) {
  observeEvent(input$classify, {
    #prediction
    our_prediction <- data.frame(text = input$text)
    our_prediction$word_count <- sapply(strsplit(as.character(our_prediction$text), "\\s+"), length)
    our_prediction$count_exclamations <- sum(grepl("!", our_prediction$text))
    our_prediction$has_exclamation <- ifelse(grepl("!", our_prediction$text), "yes", "no")

    prediction <- predict(naive_model_hints, our_prediction)
    
    # Output
    output$prediction <- renderText({
      paste("Prediction: ", prediction)
    })
  })
}
```

## running the app

```{r}
shinyApp(ui = ui, server = server)
```
